## 3. How Do Recommendations Work?

In order to fully understand the current landscape of recommendation
systems, it’s important to go through the history of the algorithmic
advancements. No method created yet is a one-size-fits-all solution, so
it’s essential to understand the benefits of all possible methods in
order to create the best system for your problem.

Furthermore, this is an active field of research, and new algorithms and
methods are constantly being published. One result of this volatility in
the field is that an understanding of the nuances of your data is
necessary in making proper algorithmic choices; no current algorithm is
robust enough to take in arbitrary data and output high-quality results.
Even the multimodal approaches we present in this report are still
fragile and require care when being used (as we discuss in
[Failures](#failures)). As a result, knowing the variety of recommendation
algorithms that are available and iterating in complexity is crucial for
building a high-quality recommendation system.

### History of Recommendation Systems

#### Collaborative Filtering

The term *collaborative filtering* first appeared in a 1992 paper
describing Tapestry,^[See https://dl.acm.org/doi/pdf/10.1145/138859.138867] an experimental mail system developed to help
users filter for interesting emails. Collaborative filtering was novel
because it introduced a new dimension to the recommendation problem:
user feedback. In addition to filtering documents by content keywords,
this made it possible to narrow down the results to documents that
others have found interesting. User feedback data can be *explicit* or
*implicit*. When a user provides explicit preference information (such
as liking or disliking an email), the data is considered explicit.
Implicit data, on the other hand, is generated by user actions from
which email preferences are inferred. One example of implicit data is
the number of times a user forwards an email.

Recommendation systems today continue to use collaborative filtering,
but the collaborative data used is more extensive, the filtering methods
more sophisticated. Given a set of rich historical interaction data, a
recommendation system attempts to tease out some information that allows
it to predict user preferences. One approach is to identify a group of
similar users from the data and assume they share the same preferences.
If Alice liked two out of the three books that Bob liked, perhaps Alice
is similar to Bob and the system can recommend the third book Bob liked
to Alice? This *neighborhood* strategy disregards the underlying
content — it does not need to know that the books Bob and Alice both
liked were from the *Harry Potter* series. On the other hand, the lack
of domain knowledge means that neighborhood methods can only predict
preferences for items they have seen before (the cold start problem). In
the example of Alice and Bob, if there is a new *Harry Potter* book and
no one has read it before, the system will not know what to do with it.

What sort of representation for users should we use such that
recommendation systems can digest it? Users and their interactions with
various items can first be visualized using a graph. In Figure 3.1, we see that there are three
users (Alice, Bob, Charlie) and four items (*Sorcerer’s Stone*, *Chamber
of Secrets*, *Python 101*, *JavaScript 101*). A line between a user and
an item means that an interaction occurred. This interaction can be an
explicit "like" or an implicit "read." If we have more information about
the interaction, that can be added as a weighting factor to the line.
For example, if a user rated an interaction on a scale of 1 to 5, the
line can be weighted accordingly.

![Figure 3.1](figures/03-01.png)
*Figure 3.1 Graph of user and item interactions.* 

In Figure 3.1 we see, for example, that
Alice likes *Chamber of Secrets*, *Javascript 101*, and *Python 101*
while Charlie likes *Chamber of Secrets* and *Python 101*. This
graphical representation needs to be transformed into a matrix before
recommendation algorithms can process it (a matrix formed like this to
show interactions is called an **adjacency matrix**). By convention,
each row of the matrix represents a user and each column of the matrix
represents an item. Figure 3.2 shows
the matrix derived from our corresponding graph. Each value of a line in
the graph is entered into the corresponding (User, Item) cell in the
matrix. In our example, the line between Alice and *Chamber of Secrets*
is entered as a 1 in cell (2,1) of the matrix, since we assign Alice to
row one and *Chamber of Secrets* to column two. Our example with three
users and four items translates into a matrix of size 3x4.

![Figure 3.2](figures/03-02.png)
*Figure 3.2 User and item interactions in matrix representation.* 

In real-life applications, these interaction matrices are large and
sparse. The [MovieLens 100K dataset](https://www.kaggle.com/datasets/prajitdatta/movielens-100k-dataset), often used for benchmarking
purposes, has 100,000 ratings from 943 users on 1,682 movies. The
corresponding matrix has a dimension of 943x1682 and is sparse; only
6.3% of the matrix has data. This sparsity makes it incredibly hard to
extract any meaningful information from the graph structure, since we
simply have no clue whether the other 93.7% of interactions would go
well or poorly.

Furthermore, these matrices usually contain weak preference signals.
Imagine a user who rates many movies highly and indiscriminately. This
user would be connected to almost every movie in the catalogue and
would have a disproportionate effect on the evaluation of
recommendations. In general, it is said that collaborative filtering is
not very "spam-resistant" for this reason; popular items or users will
unduly influence any future recommendations. At the other extreme, a
user who rates very sparingly does not give our system very much
information about their preferences. As a result, their recommendations
will be more susceptible to the spammy data.

##### k-Nearest Neighbor (k-NN)

Now suppose we would like to make a recommendation for Bob. We could
start by determining who is similar to Bob. One reasonable metric to use
is "the number of items that both users interacted with." Would this
work? Imagine two cases: in the first case, Frank and Grace rated a
total of 100 items, and 2 of the items are the same; in the second case,
Frank and Dan rated a total of 5 items and 2 of the items are the same.
Our original metric would suggest that Frank is equally similar to Grace
and Dan. Clearly this is not true. For our original metric to be useful,
it needs to be *normalized*. We divide by the total number of items
interacted with by both users to obtain a new metric.

![Figure 3.3](figures/03-03.png)
*Figure 3.3 Which similarity score is chosen for collaborative filtering can substantially change the outcomes.* 

Using this new metric to measure similarity (or *distance*), we compute
the distance between (Alice, Charlie) to be 2/3 and between (Bob,
Charlie) to be 1/3. Since Charlie is more similar to Alice, we assume
that Charlie will also be interested in what Alice has liked. Looking
back to the matrix representation, we recommend *JavaScript 101* to
Charlie.

![Figure 3.4](figures/03-04.png)
*Figure 3.4 Dividing by nearest neighbors helps us understand which other user is most similar to Charlie.* 

The *k*-NN algorithm is based on this idea. Instead of finding the most
similar user (or "neighbor"), it looks for *k* of them. In addition, the
algorithm uses various distance measures.^[Euclidean distance (straight-line distance between two points) is
commonly used for continuous variables and Hamming distance (number of
positions at which the strings are different) for discrete variables.] In the context of
recommenders, one can think of *k* as a trade-off between precision and
generality. A large *k* implies that the recommendation result is
obtained by aggregating preference information for many similar
users.^[Examples of ways to aggregate information across users are *mean* and *median*.] As a result, it is less targeted and more general. In the
extreme case where *k*=1, the system relies on the preference of a
single user.

The algorithm, while simple to implement, is computationally expensive
because it calculates a distance measure for all users in the training
set.^[This makes the algorithm *O*(*N*^2).] For large datasets, it is often intractable. On the other hand,
if the dataset is very small, simple methods like *k*-dimensional
(*k*-d) trees would suffice. Here, as the name implies, data is split
(approximately) in half along each dimension. To find the nearest
neighbor of a data point, one just needs to walk down the tree.
Unfortunately, like *k*-NN, the *k*-d tree algorithm suffers from the
curse of dimensionality; the large number of subbranches in the tree
makes finding the nearest neighbor prohibitive.

##### Locality-Sensitive Hashing (LSH) Forest

Instead of finding **the** nearest neighbor, getting an **approximate**
nearest neighbor often suffices in real-life applications. Approximate
algorithms are stochastic. Even though the solutions are
not exact and are correct only to a certain probability, these
algorithms allow us to trade off accuracy with speed. LSH Forest is an
example of such an algorithm.^[See the paper here: http://infolab.stanford.edu/~bawa/Pub/similarity.pdf or, for a fantastic short talk, check this out instead: https://www.youtube.com/watch?v=kKRvEJrvvso]

![Figure 3.5](figures/03-05.png)
*Figure 3.5 LSH groups datapoints into buckets.* 

Given a set of data points, LSH attempts to group (through a hash
function) points that are close together into one bucket, as illustrated
in Figure 3.5. Points that end up in the same bucket
as the target point are considered similar; their actual distance
measures to the target are computed. The top *k* (as in *k*-NN) points
are returned. The probabilistic nature of LSH implies that each run of
the algorithm can produce different outcomes — that is, each time the
algorithm is run, it can find different sets of points that belong in
the same bucket as the target. To increase the chances of finding more
similar points, the algorithm is run multiple times, with each run
yielding a set of points that are close to the target (hence the name
"Forest"). The top *k* points are selected from the union of these sets.

#### Matrix Factorization

Up until now we have been discussing recommendation methods based on
user similarities. These *neighborhood* approaches do not scale well to
larger datasets and lack understanding of the underlying data. Given a
user-book interaction matrix, neighborhood approaches can tell you that
Bob and Alice have similar interests, but are unable to explain that Bob
and Alice are similar *because they both like young adult fantasy
books*.

This lack of understanding about the underlying reasons for an
interaction contributes to collaborative filtering’s inability to deal
with spammy data. It also ruins any potential to deal with the cold
start problem, since without interactions with an item or user, we have
no information about it. On the other hand, matrix factorization, which
belongs to a class of *latent factor* models, is a different approach
that tries to make sense of the interaction matrix. It does so by
finding factors that explain most or all of the information in the
matrix. These factors are extracted from the data mathematically and do
not easily map to humanly noticeable ones.^[In examples throughout this report we use the genre of a book as an example of these factors for illustrative reasons. Actual factors will not be as human-interpretable.]

![Figure 3.6](figures/03-06.png)
*Figure 3.6 Matrix factorization uses two matrices. The first showing user preference for factors, and the second showing item associations with those factors.* 

Recall that in collaborative filtering, the representation is the
interaction matrix where each row represents a user and each column
represents an item. In matrix factorization we instead use two smaller
matrices to represent this data. The first matrix contains user
preferences for factors (rather than items), and the second matrix
associates items with their factor representations. As a hypothetical
example, instead of liking all 14 *Harry Potter* and *Twilight* books, a
user now likes the "Fantasy" and "Young Adult" factors. *Chamber of
Secrets* is now represented by the factors "Fantasy" and "Young Adult."
The hope is that this new representation can model underlying features
in the data and start approaching the question of why a user liked a
particular item. If our representation can somehow encode this, then our
recommendation algorithm can use this information in its predictions.

Once a factorized version of the interaction matrix is created, there
are two ways to use it for recommendations. In the first approach, we
multiply these two matrices to give us an approximation of the original;
issuing a recommendation for a particular user-item pair just means
reading off the corresponding row and column from the new matrix^[Again, each user corresponds to a particular row and each item to a
particular column in the matrix.] 
In the second (more common) approach, the problem reverts back to a
*k*-nearest neighbors problem of finding items with similar
representations (or in this case, factor values) to users. However, the
output of the recommendation system is now a set of factors relating to
preference, which needs to be mapped to items. Just like in
collaborative filtering, where we need a method to map similar users to
ranked items, we now need a way to map user preferences to items.

![Figure 3.8](figures/03-08.png)
*Figure 3.8 We can recommend books to Bob based on his factor preferences. Note that he has already read* Sorcerer’s Stone *and* Chamber of Secrets *so we filter those from the final recommendation.* 

To use matrix factorization, the number of factors needs to be
specified. A smaller number is preferred because the resulting
user-factor and item-factor matrices will be small. This leads to a
considerable speedup for both training the model and offering
recommendations. Because matrix factorization enables one to model the
interaction matrix, it is sometimes referred to as *model-based*
collaborative filtering.

##### Nonnegative Matrix Factorization (NMF)

NMF^[https://arxiv.org/abs/1010.1763] is a special kind of matrix factorization used when the original
interaction matrix does not have negative elements and both smaller
decomposed matrices are required to be nonnegative. In certain
applications, the nonnegativity constraint is preferred because it
provides physically meaningful features. An example is an interaction
matrix that captures the number of times a user clicked on an item. The
original matrix has physical meaning associated with it, and NMF
preserves the nonnegativity of the resulting factor matrices.

In NMF, the decomposition of the original matrix into two smaller ones
is obtained by first defining a cost function^[One possibility is to use the Frobenius distance, an extension of the Euclidean distance to matrices. In practice, regularization parameters are added to the cost function to avoid overfitting.] and subsequently
minimizing it, often using stochastic gradient descent (SGD). This
algorithm is an example of why understanding the data we are basing our
algorithms on is so important. Matrix factorization algorithms, like
many algorithms based solely on interaction data, can have trouble
converging. Giving our algorithm additional information about the form
and constraints of our data, like the fact that no entries will ever be
negative, helps it converge and come to a sensible result.

##### Singular Value Decomposition (SVD)

Another algorithm often discussed along with matrix factorization is
SVD. Conventional SVD of a matrix factorizes it into a product of three
matrices. In the context of an interaction matrix, one can interpret the
first matrix as describing the interaction between users and features,
the second as describing the interaction between items and features, and
the third as a weighting matrix. In contrast to NMF, conventional SVD is
a theoretical solution that gives an exact decomposition and is only
defined for a fully specified matrix. Given that most historical rating
matrices were sparse, early recommendation systems had to replace
missing data with substitute data in order to use SVD. This distorted
the data and increased computational complexity. As a result,
conventional SVD is not commonly used in recommendation systems today.

Simon Funk’s version of SVD (of Netflix Prize fame) is a spin on the
traditional, where only observed historical data is fitted.^[The solution is obtained by using gradient descent to minimize a cost function based on squared error between the observed and estimated
ratings. In practice, regularization and bias terms are added to the
cost function.] By
fixing the number of features used to estimate the original interaction
matrix, the algorithm works to decompose it into two smaller matrices.
Today when SVD is mentioned in the literature, it generally refers to
this modified version. When a nonnegativity constraint is not important,
SVD is a popular algorithm because of its accuracy and scalability.

##### Principal Component Analysis (PCA)

![Figure 3.7](figures/03-07.png)
*Figure 3.7 PCA analysis extracts the directions with the most variability in the data.*

Another algorithm that can be used to extract features to represent the
interaction matrix is PCA. To understand PCA, take a look at
Figure 3.7, which shows some data points plotted in a
two-dimensional space. The direction (or vector) that explains the most
variability in the data is denoted by the longer orange line. The
remaining variability is explained by the shorter orange line. PCA works
by finding and ranking these vectors; they are called "principal
components" and can be used to recover the original data. PCA is useful
when one is trying to identify important components and ignore noise in
the data. This can be achieved by using the top principal components
(instead of all) to build an estimator for the original dataset.

Using PCA in a recommendation system is straightforward — it is applied
to the interaction matrix and the top *k* principal components are used
to construct an approximation of the historical matrix. Similar to the
number of factors in NMF and the number of features in SVD, the number
of components in PCA is a knob that can be adjusted to trade off the
amount of information used and computation tractability. In practice, it
is optimized along with other hyperparameters.

#### Extensions to Factorization and Collaborative Filtering

Of the two approaches we’ve discussed, matrix factorization algorithms
are more widely adopted because they scale well to large datasets.
Unfortunately, both approaches are unable to make predictions for users
with very few ratings — but when we shift gears and model user feedback
in a probabilistic setting (vs. deterministic, as we have been doing so
far), it is possible to find a solution that scales, works well on
sparse and imbalanced datasets, and performs well when offering
predictions for users with few ratings.

Similar to other matrix factorization methods, we start by assuming that
user preferences are determined by a small number of unobserved factors
and that items are represented by the same set of factors. The original
interaction matrix can then be decomposed into two smaller feature
matrices, a user-feature matrix and an item-feature matrix. As implied
by their names, the user-feature matrix provides information on the
interaction between users and features, and the item-feature matrix
provides information on the interaction between items and factors. A
user-item interaction is just a linear combination of the two feature
matrices. In previous matrix factorization models these feature matrices
are deterministic; in a probabilistic setting we assume they follow some
probabilistic distribution. As a result, the observed interaction is
also probabilistic, and its outcome is dependent on the two matrices.

An example of such an approach is probabilistic matrix factorization
(PMF).^[https://www.cs.toronto.edu///~amnih/papers/pmf.pdf] In PMF, we first assume that the user-feature and
item-feature matrices both follow a Gaussian process. Given the observed
interactions, PMF finds the parameters that make the two feature
matrices most probable in explaining these interactions.^[In PMF, we assume both U and V follow a Gaussian process with zero
mean (in other words, they have Gaussian priors). The likelihood of
observing a rating given (U, V) is also Gaussian. When we fix the
observation noise variance and prior variances, the maximization with
respect to (U, V) is equivalent to minimizing the sum of squares error
function with quadratic regularization terms.]
Interestingly, this approach can be thought of as the probabilistic
extension of SVD, since the formulation reduces to SVD if all ratings
can be observed. PMF has been shown to perform well on large, sparse,
and imbalanced datasets where some users have interacted with many items
and others only a few items.

To add to the model zoo, even PMF has many extensions to it. Bayesian
PMF,^[https://www.cs.toronto.edu/~amnih/papers/bpmf.pdf] for example, provides distributions instead of point estimates
for the feature matrix. This means that we have a sense of uncertainty
for the resulting user-item preferences and can use that uncertainty
either to make more motivated recommendations or to help our system know
the best questions to ask our users to get a better understanding of
them. Also, based on of the success of neural networks' incredibly
nonlinear systems, there is *nonlinear matrix factorization*,^[http://people.ee.duke.edu/\~lcarin/MatrixFactorization.pdf] which
aims to capture more robust features from the interaction data.

##### Hybrid Collab/Factorization Methods

Recommendation systems based on collaborative filtering and matrix
factorization suffer from the cold start problem — they only work on
items users have interacted with. For example, news article
recommendation systems cannot recommend an article unless enough people
have interacted with it and the system has been rerun to include these
interactions. In addition, collaborative filtering methods in particular
are affected by the sparsity problem, since these methods need enough
data to be able to determine similarity between users. Items interesting
to a niche group are difficult to recommend; items liked by many are
overly recommended.

What if we add more information to the recommendation system, so that it
understands what an item is, and what a user is? Instead of representing
*Chamber of Secrets* as item *i*, we can represent it using two
features, "Young Adult" and "Fantasy." Similarly, we can represent Bob
using "Male" and "Age 30." The interaction matrix no longer contains
interactions between users and items, but interactions between user
features (in each row) and item features (in each column).^[These features can be fitted numerically (using regression, for example) or manually encoded.] With this
new interaction matrix, we can proceed to use either collaborative
filtering or matrix factorization to obtain recommendations. Since the
resulting recommendations are for features, they need to be converted to
item recommendations via mapping and aggregation.

The hybrid system alleviates both the cold start and sparsity problems.
When a new user or item enters the system, it is no longer "new" because
it can be represented by the existing user or item features in the
recommendation system. In addition, because many items now map onto a
common set of item features (and similarly, many users onto a set of
corresponding user features), the interaction matrix is more dense and
results are explainable.

##### Adding Heuristic Combinations of Things

It’s important to note that by adding metadata to the underlying
recommendations, additional information can be considered in the
prediction after the fact.

A common way to do this is by calculating a secondary score for the
metadata that is desired and multiplying that by the score calculated
with the recommendation system. For example, if we wish to constrain
recommendation results to be geographically close to the user, we can
calculate a score describing how close a result is to the chosen
location. However, it is important to make a motivated decision about
how this score is normalized.

If, for example, we normalize all the geographic scores from [0, 1),
then geography can never increase the value for a particular item. So,
if we are showing these scores to the user we must account for the fact
that this geography score will, by definition, lower the values for all
items. On the other hand, if we only require the score to be positive,
then we must be aware that sometimes we will recommend suboptimal items
simply because they are close to the chosen location. This would result
in recommending a restaurant that is geographically close to a user,
regardless of whether or not they would like it.

Generally, the solution to this is training a recommendation system
without knowledge of geography, or whatever the secondary metadata is,
and then fitting a separate model to learn how to combine the scores.

#### Neural Network Approaches

Deep learning’s success when applied to visual and speech recognition
problems has motivated practitioners and researchers to use neural
networks for recommendations.^[See https://arxiv.org/abs/1707.07435 and https://dlrs-workshop.org/] Neural networks are promising because
they can not only handle historical interaction data, but easily process
unstructured information such as text, images, audio, or even things as
abstract as how an object moves. In addition, neural networks are
powerful at doing their own feature engineering in order to figure out
for themselves what is important or not within that data. This, coupled
with their nonlinearity, helps them uncover the relationships between
objects.

In practice, these networks can be used as standalone systems or
combined with traditional techniques to achieve better performance. For
example, we can use embeddings to create deep representations for
objects which can then be used with a *k*-NN algorithm to find the k
best recommendations.
Alternatively, session-based networks using recurrent neural networks
can directly predict the next object that should be recommended without
the use of auxiliary algorithms.

##### Embeddings

Embeddings and matrix factorization share the point of view that we
should enrich the user-item representations we use in our recommendation
systems with metadata.^[https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html] However, one problem with matrix
factorization methods is that they generally can only extract global
structure from the problem. For books, that would mean they can extract
some sense of genre, but won’t understand how books within the same
genre differ and interact.

This is where modern embedding models thrive: they are able to
understand not only global structure (i.e., how various genres compare
to one another) but also structure on a smaller scale (i.e., how various
books within a genre compare to one another). Furthermore, the way they
do this is consistent at all levels. Because of this, we are able to
think of the space that they create as a semantic space — it encodes
deep features about the objects in a feature-rich way that can be used
to exploit their fundamental properties.

![Figure 3.10](figures/03-10.png)
*Figure 3.10 Word2vec encodes words as vectors with local and global structure, allowing word analogies to be solved through arithmetic.*

Turning back to our work in [Summarization](https://summarization.fastforwardlabs.com/) as an example, the word embeddings we
discuss there create a space where all words relating to capitals of
countries are in the same region as one another, and this cluster is
close to the cluster that contains all of the country names. At the same
time, the path to get from the point for "United States" to the point
for "Washington, DC" is similar to the one from "France" to "Paris" and,
to some degree, from "New York" to "Albany." This is the sort of rich
structure we expect from a semantic embedding space.

In that report, we also go through how to train such a semantic
embedding space on text. The training procedure for recommendations is
quite similar. We can think of each word in our text model as
corresponding to an object from our recommendations dataset, and each
sentence as corresponding to the set of objects a single user has
interacted with. Each object, however, is represented by some
description of it. As a result, each user is represented by a sequence
of descriptions.

![Figure 3.11](figures/03-11.png)
*Figure 3.11 For recommendations, a user can be embedded as a sequence of descriptions of items they interacted with.*

This is where things start changing, because we are using a multimodal
model instead of a unimodal model. Our two modalities are items and
users, and they can both be represented in different ways using
different types of data.^[In the [Prototype](prototype) section we discuss how we represent a book by its summary and a user by the sequence of books, each represented by its own summary, that the user has reviewed. This choice was made for simplicity, and in reality the representations used for both objects can be completely different.] In order to do this, we pick a user
representation, remove any reference to a random selection of objects in
that user’s history, and create a single encoding for that user.
Separately, we encode one of the items that we removed. The model is
rewarded if the user encoding and the item encoding are close to one
another. At the same time, we also randomly sample items that user has
never interacted with and penalize the model if the user-sequence
encoding and the random-item encoding are close to one another.

![Figure 3.12](figures/03-12.png)
*Figure 3.12 Example of how a sequence of user interactions is turned into multiple training samples for our multi-modal model.* 

With these embeddings fully trained, we are able to make recommendations
using standard *k*-nearest neighbor approaches by simply encoding a
user’s history and finding items that encode to similar values. Another
benefit of a system like this is that it is truly flexible and allows
all sorts of additional usages. Items or users can be added or
subtracted in order to allow for better exploration of the catalogue,
and users can be recommended to users based on similar preferences. The
system provides a general way of understanding how users and items
interact with one another in a consistent way.

Furthermore, the system is able to find deep features within the objects
and embed them, purely based on their raw data. The cold start problem
is no longer an issue, because the only input for the recommendation
system is the metadata or description of the item. This method is
powerful enough to be used as a recommendation system whose input is raw
audio signals!^[See https://arxiv.org/abs/1706.09739]

##### Session-Based Methods

Session-based deep learning methods try to use neural networks to help
in cases when users don’t necessarily interact with many items. For
example, if I have a large e-commerce website and dozens of interactions
for each user in my dataset, matrix factorization may do a good job of
extracting some relevant features about their preferences. However, if I
have a small e-commerce website without much history for my users,
matrix factorization simply won’t be able to extract enough information
from my dataset to be useful.

Another appeal of session-based methods is how some variants can take
into account the temporal nature of session-based data. For example,
they can see that a user is first reading a general algorithms book,
then a book specifically on neural networks, and use that trajectory to
recommend a good next step.

### What This All Means

Clearly, the scope of recommendation algorithms is quite wide; however,
they all share a common scheme: find some representation for the objects
you are recommending and then, to make a recommendation for a given
object, find objects with a similar representation. While the methods
described here may seem to vary greatly, from the simplicity of
collaborative filtering to the complexity of multimodal embeddings, they
all follow this same pattern; the question is which one can make a
robust enough representation for your data.

It’s important to be aware of this breadth because at this point in the
field of recommendations there is no way to know just by looking at your
data which method will return the best results. Therefore, when building
any recommendation system, it is necessary to start with the simplest
approach and work your way up in complexity, testing at every step to
make sure that the additional complexity is in fact helping your system
better understand your data. In [Model](#model), we discuss the
multimodal model we chose, how we evaluated it, and how we compared it
to simpler approaches.